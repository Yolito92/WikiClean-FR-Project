
# ğŸ‡«ğŸ‡· Zeronex â€“ Wikipedia FR Clean Dataset (Extrait )
### DÃ©monstration de nettoyage, normalisation et structuration dâ€™un corpus massif

Ce dÃ©pÃ´t contient **un extrait reprÃ©sentatif (~0.0001)** du travail rÃ©alisÃ© sur le dump complet de WikipÃ©dia FR.  
Cet extrait a pour but de **montrer la qualitÃ© du nettoyage**, la cohÃ©rence du format final et lâ€™exploitabilitÃ© immÃ©diate pour les applications IA (NLP, RAG, embeddings, graph databasesâ€¦).

Le dataset complet comprend **2,7 millions dâ€™articles**, mais seule une fraction est publiÃ©e ici.

---

## ğŸ§  Objectif du projet

Transformer le contenu brut de WikipÃ©dia FR, naturellement complexe et peu exploitable, en un dataset :

- propre  
- stable  
- homogÃ¨ne  
- lisible  
- structurellement cohÃ©rent  
- prÃªt Ã  Ãªtre utilisÃ© dans nâ€™importe quel pipeline IA

Lâ€™extrait inclus dÃ©montre la capacitÃ© Ã  :

- nettoyer et normaliser le texte (suppression du wikitext, HTML, templates inutilesâ€¦)  
- restructurer les infobox en paires clÃ©/valeur  
- isoler proprement les rÃ©fÃ©rences  
- extraire les liens internes  
- organiser les catÃ©gories  
- produire un JSON clair, portable et rÃ©utilisable  

---

## ğŸ“Š Contenu de lâ€™extrait (~10%)

Chaque fichier JSON prÃ©sent dans ce dÃ©pÃ´t suit une structure stable contenant :

- **title** â€” titre de lâ€™article  
- **text_clean** â€” texte principal nettoyÃ©  
- **infobox** â€” informations structurÃ©es lorsquâ€™elles existent  
- **links** â€” liens internes extraits  
- **references** â€” rÃ©fÃ©rences externes isolÃ©es  
- **categories** â€” catÃ©gories dâ€™origine  
- **timestamp** â€” date de rÃ©vision  
- **revision_id** â€” identifiant de la rÃ©vision  

Cet extrait est fourni **exclusivement pour illustrer lâ€™efficacitÃ© du processus**.

---

## ğŸŸª Pourquoi seulement 10% ?

Ce repository ne vise pas Ã  redistribuer la totalitÃ© du dump WikipÃ©dia FR, mais Ã  :

- prÃ©senter la **mÃ©thode de transformation**  
- montrer la **qualitÃ© du nettoyage**  
- dÃ©montrer la **stabilitÃ© du format**  
- fournir une **preuve de concept exploitable**  
- permettre des tests rÃ©els sur une partie reprÃ©sentative du corpus  

Le dataset complet (2,7M articles) dÃ©passe le cadre dâ€™une release GitHub classique.

---

## ğŸŒ Usages possibles de lâ€™extrait

Cet extrait permet dÃ©jÃ  de tester :

- pipelines NLP  
- modÃ¨les RAG  
- moteurs de recherche  
- embeddings  
- extraction dâ€™entitÃ©s (NER)  
- bases de graphes  
- indexation full-text  
- QA systems en franÃ§ais  

Sans nÃ©cessiter le dataset complet.

---

## ğŸ§© Philosophie

Rendre le savoir francophone plus accessible aux systÃ¨mes modernes dâ€™IA en proposant un format :

- Ã©purÃ©  
- standardisÃ©  
- polyvalent  
- immÃ©diatement utilisable  

Ce projet sâ€™inscrit dans une dÃ©marche dâ€™ouverture, de clartÃ© et dâ€™amÃ©lioration continue.

---

## ğŸ“¦ Format du dataset (extrait)

Chaque fichier suit la structure :

```json
{
  "title": "",
  "text_clean": "",
  "infobox": {},
  "links": [],
  "references": [],
  "categories": [],
  "timestamp": "",
  "revision_id": ""
}
